{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7381\n",
      "                                                   path  quality part  \\\n",
      "0     .\\data\\data\\source_data\\bumper\\scratch\\205_101...        1   범퍼   \n",
      "1     .\\data\\data\\source_data\\bumper\\scratch\\205_101...        1   범퍼   \n",
      "2     .\\data\\data\\source_data\\bumper\\scratch\\205_101...        1   범퍼   \n",
      "3     .\\data\\data\\source_data\\bumper\\scratch\\205_101...        1   범퍼   \n",
      "4     .\\data\\data\\source_data\\bumper\\scratch\\205_101...        1   범퍼   \n",
      "...                                                 ...      ...  ...   \n",
      "1044  .\\data\\data\\source_data\\frame\\Seam_failure\\207...        0  프레임   \n",
      "1045  .\\data\\data\\source_data\\frame\\Seam_failure\\207...        0  프레임   \n",
      "1046  .\\data\\data\\source_data\\frame\\Seam_failure\\207...        0  프레임   \n",
      "1047  .\\data\\data\\source_data\\frame\\Seam_failure\\207...        0  프레임   \n",
      "1048  .\\data\\data\\source_data\\frame\\Seam_failure\\207...        0  프레임   \n",
      "\n",
      "                                                 bboxes  \n",
      "0                                                    []  \n",
      "1                                                    []  \n",
      "2                                                    []  \n",
      "3                                                    []  \n",
      "4                                                    []  \n",
      "...                                                 ...  \n",
      "1044  [[1910.5116279069766, 762.874251497006, 240.27...  \n",
      "1045  [[1857.7674418604647, 991.7365269461079, 205.1...  \n",
      "1046  [[1807.3674060910248, 833.2934131736528, 662.2...  \n",
      "1047  [[2059.367406091025, 657.2455089820359, 287.16...  \n",
      "1048  [[2006.3731264331855, 687.1101077368886, 119.2...  \n",
      "\n",
      "[7381 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data_with_pandas(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "data_bumper = load_data_with_pandas('./data/data/output/bumper/bumper_data.csv',)\n",
    "data_door_ed = load_data_with_pandas('./data/data/output/door/door_ed_data.csv')\n",
    "data_door_scratch = load_data_with_pandas('./data/data/output/door/door_scratch_data.csv')\n",
    "data_fender = load_data_with_pandas('./data/data/output/fender/fender_ed_data.csv',)\n",
    "data_frame_ed = load_data_with_pandas('./data/data/output/frame/frame_ed_data.csv')\n",
    "data_frame_hd = load_data_with_pandas('./data/data/output/frame/frame_hd_data.csv')\n",
    "data_frame_sealf = load_data_with_pandas('./data/data/output/frame/frame_sealf_data.csv')\n",
    "data_frame_seamf = load_data_with_pandas('./data/data/output/frame/frame_seamf_data.csv')\n",
    "\n",
    "\n",
    "# 데이터를 하나의 데이터프레임으로 결합\n",
    "data = pd.concat([\n",
    "    data_bumper,\n",
    "    data_door_ed,\n",
    "    data_door_scratch,\n",
    "    data_fender,\n",
    "    data_frame_ed,\n",
    "    data_frame_hd,\n",
    "    data_frame_sealf,\n",
    "    data_frame_seamf\n",
    "], axis=0)  # axis=0: 행 방향으로 결합\n",
    "\n",
    "# 결과 출력\n",
    "print(len(data))  # 총 데이터 개수\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 데이터 개수\n",
    "total_data = len(data)  # 예를 들어 7400개\n",
    "batch_size = 100  # 각 배치 크기\n",
    "\n",
    "# 배치 수 계산\n",
    "num_batches = total_data // batch_size\n",
    "\n",
    "# 중복되지 않는 랜덤 샘플을 추출하여 배치로 나누기\n",
    "random_indices = random.sample(range(total_data), total_data)  # 전체 데이터를 중복 없이 랜덤하게 섞기\n",
    "\n",
    "# 데이터 나누기\n",
    "batches = [random_indices[i * batch_size: (i + 1) * batch_size] for i in range(num_batches)]\n",
    "\n",
    "# 마지막 배치가 200개보다 적을 경우 처리\n",
    "if total_data % batch_size != 0:\n",
    "    batches.append(random_indices[num_batches * batch_size:])\n",
    "\n",
    "# 결과 확인\n",
    "# for i, batch in enumerate(batches):\n",
    "    # print(f\"Batch {i + 1}: {len(batch)} items\")\n",
    "\n",
    "print(batches[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import ast  # 문자열을 리스트로 변환하기 위해 사용\n",
    "\n",
    "# 이미지 데이터 로드 및 배열 생성\n",
    "x = []  # 이미지 데이터를 저장할 리스트\n",
    "y = []  # 라벨을 저장할 리스트\n",
    "\n",
    "# 각 배치에 대해 이미지 데이터 로드\n",
    "count = 0  # 배치 완료 횟수를 추적\n",
    "for batch in batches:\n",
    "    y_batch = []\n",
    "    x_batch_imgs = []  # 이미지 배열 저장\n",
    "    x_batch_parts = []  # 부품명 저장\n",
    "    x_batch_bboxes = []  # bbox 저장\n",
    "\n",
    "    for i, idx in enumerate(batch):\n",
    "        img_path = data.iloc[idx, 0]  # 이미지 경로\n",
    "        label = data.iloc[idx, 1]    # 라벨\n",
    "        part = data.iloc[idx, 2]     # 부품명 로드\n",
    "        bbox_str = data.iloc[idx, 3]  # bbox 문자열 로드\n",
    "\n",
    "        # bbox 문자열을 리스트로 변환\n",
    "        try:\n",
    "            bbox = ast.literal_eval(bbox_str)  # 문자열을 리스트로 변환\n",
    "            if not isinstance(bbox, (list, tuple)):\n",
    "                raise ValueError  # 리스트나 튜플이 아닌 경우 예외 처리\n",
    "        except (ValueError, SyntaxError):\n",
    "            bbox = []  # 변환 실패 시 빈 리스트로 처리\n",
    "\n",
    "        img = Image.open(img_path)   # 이미지 로드\n",
    "        img_array = np.array(img)    # 이미지 배열로 변환\n",
    "\n",
    "        # 데이터 추가\n",
    "        x_batch_imgs.append(img_array)\n",
    "        x_batch_parts.append(part)\n",
    "        x_batch_bboxes.append(bbox)\n",
    "        y_batch.append(label)        # 배치에 라벨 추가\n",
    "\n",
    "        # 진행 상황 출력 (배치 내에서만)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"배치 진행현황: {int(i / len(batch) * 100)}% 완료\")\n",
    "\n",
    "    count += 1  # 배치 완료 횟수 증가\n",
    "    print(f\"{count}회 완료\")\n",
    "    # 각 배치 완료 후 x와 y에 추가\n",
    "    x.append([np.array(x_batch_imgs), x_batch_parts, x_batch_bboxes])\n",
    "    y.append(np.array(y_batch))\n",
    "    \n",
    "\n",
    "# x와 y를 하나로 결합 (배치 단위로)\n",
    "x = np.concatenate(x, axis=0)  # 배치 단위로 이미지를 결합\n",
    "y = np.concatenate(y, axis=0)  # 배치 단위로 라벨을 결합\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"x.shape: {x.shape}\")  # 이미지 배열 크기\n",
    "print(f\"y.shape: {y.shape}\")  # 라벨 배열 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# # 이미지 데이터 로드 및 배열 생성\n",
    "# first_image_path = data.iloc[0, 0]  # 첫 번째 이미지 경로\n",
    "# img_1 = Image.open(first_image_path)\n",
    "# a = np.array(img_1)\n",
    "# x = np.expand_dims(a, axis=0)  # 첫 번째 이미지 배열 확장\n",
    "# y = []  # 라벨을 저장할 리스트\n",
    "\n",
    "# # 이미지 데이터 배치 생성\n",
    "# for i, idx in enumerate(num):\n",
    "#     img_path = data.iloc[idx, 0]  # 이미지 경로\n",
    "#     label = data.iloc[idx, 1]    # 라벨\n",
    "#     img = Image.open(img_path)   # 이미지 로드\n",
    "#     img_array = np.array(img)    # 이미지 배열로 변환\n",
    "#     img_expanded = np.expand_dims(img_array, axis=0)  # 차원 확장\n",
    "#     x = np.concatenate((x, img_expanded), axis=0)     # 이미지 데이터 결합\n",
    "#     y.append(label)                                   # 라벨 추가\n",
    "\n",
    "#     # 진행 상황 출력\n",
    "#     if i % 10 == 0:\n",
    "#         print(f\"진행현황: {int(i / data_num * 100)}% 완료\")\n",
    "\n",
    "# # 첫 번째 이미지 제거\n",
    "# x = np.delete(x, 0, axis=0)\n",
    "# y = np.array(y).reshape(data_num, 1)  # 라벨 배열로 변환\n",
    "\n",
    "# # 결과 출력\n",
    "# print(f\"x.shape: {x.shape}\")  # 이미지 배열 크기\n",
    "# print(f\"y.shape: {y.shape}\")  # 라벨 배열 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 이름에 따라 데이터 분리\n",
    "image_paths = data['image_path']  # 이미지 경로\n",
    "labels = data['label']           # 라벨\n",
    "part_names = data['part_name']   # 부품 이름\n",
    "bbox_data = data['bbox']         # bbox 데이터 (문자열 형태로 저장됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "num = 0\n",
    "# 저장할 파일 경로\n",
    "data_save_path = f'./data/sampled_data{num}.pkl'\n",
    "\n",
    "# 데이터 저장 함수\n",
    "\n",
    "\n",
    "def save_data(file_path, x, y):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump({'x': x, 'y': y}, f)\n",
    "    print(f\"데이터가 '{file_path}'에 저장되었습니다.\")\n",
    "\n",
    "# 데이터 로드 함수\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f\"데이터가 '{file_path}'에서 로드되었습니다.\")\n",
    "    return data['x'], data['y']\n",
    "\n",
    "# 데이터 로드 후 샘플링과 분할 처리 (최초 1회 실행)\n",
    "\n",
    "\n",
    "def process_and_save_data():\n",
    "    # 이미지와 라벨 데이터 생성 (예시 코드 연결)\n",
    "    # x와 y는 이미지를 처리한 결과\n",
    "    x_sampled, y_sampled = resample(x, y, n_samples=100, random_state=42)\n",
    "    num += 1\n",
    "    # 데이터 저장\n",
    "    save_data(data_save_path, x_sampled, y_sampled)\n",
    "\n",
    "    return x_sampled, y_sampled\n",
    "\n",
    "# 데이터 로드 및 분할\n",
    "\n",
    "\n",
    "def load_and_split_data(file_path):\n",
    "    x_sampled, y_sampled = load_data(file_path)\n",
    "\n",
    "    # 데이터 분할\n",
    "    x_training, x_test, y_training, y_test = train_test_split(\n",
    "        x_sampled, y_sampled, test_size=0.2, shuffle=True, stratify=y_sampled, random_state=34\n",
    "    )\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "        x_training, y_training, test_size=0.2, shuffle=True, stratify=y_training, random_state=34\n",
    "    )\n",
    "\n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test\n",
    "\n",
    "# 데이터 처리 및 저장 (최초 1회 실행)\n",
    "# x_sampled, y_sampled = process_and_save_data()\n",
    "\n",
    "\n",
    "# 저장된 데이터 로드 및 분할\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = load_and_split_data(\n",
    "    data_save_path)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"x_train.shape: {x_train.shape}\")\n",
    "print(f\"x_valid.shape: {x_valid.shape}\")\n",
    "print(f\"x_test.shape: {x_test.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_valid.shape: {y_valid.shape}\")\n",
    "print(f\"y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# data sampling\n",
    "x_sampled, y_sampled = resample(x, y, n_samples=100, random_state=42)\n",
    "\n",
    "print(x_sampled.shape)\n",
    "print(y_sampled.shape)\n",
    "\n",
    "\n",
    "# data split\n",
    "x_training, x_test, y_training, y_test = train_test_split(\n",
    "    x_sampled, y_sampled, test_size=0.2, shuffle=True, stratify=y_sampled, random_state=34)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_training, y_training, test_size=0.2, shuffle=True, stratify=y_training, random_state=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 코드 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "model = YOLO(\"yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 감소 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.grid()\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "pred=model(y_test)\n",
    "\n",
    "\n",
    "# 혼동 행렬\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=pred)\n",
    "print('혼동 행렬: ', confmat)\n",
    "\n",
    "\n",
    "# 정확도\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=pred)\n",
    "print('정확도: ', accuracy)\n",
    "\n",
    "\n",
    "# 정밀도(precision)\n",
    "precision = precision_score(y_true=y_test, y_pred=pred)\n",
    "print('정밀도: ', precision)\n",
    "\n",
    "\n",
    "# 재현율(recall)\n",
    "recall = recall_score(y_true=y_test, y_pred=pred)\n",
    "print('재현율: ', recall)\n",
    "\n",
    "\n",
    "# roc auc\n",
    "fpr, tpr, _ = roc_curve(y_true=y_test, y_pred=pred)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('FP Rate')\n",
    "plt.ylabel('TP Rate')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('auc 점수: ', roc_auc_score(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
